<div align="center">
  <img src="logo.png" width="200" height="200">
  <h1>âœ¨ CoReader ğŸš€</h1>
  
  ![lisense](https://img.shields.io/github/license/EasonTechno/CoReader?color=blue)
  ![lisense](https://img.shields.io/badge/Eason-Techno-blue)

  [ğŸ‡ºğŸ‡¸ English](README-us.md) | ğŸ‡¨ğŸ‡³ ä¸­æ–‡ | [ğŸ‡ªğŸ‡¸ EspaÃ±ol](docs/README-es.md)
</div>

## ğŸ“‘ å†…å®¹ç›®å½•
- [ğŸŒŸ ç®€ä»‹](#-ç®€ä»‹)
- [ğŸš€ å¿«é€Ÿä¸Šæ‰‹](#-å¿«é€Ÿä¸Šæ‰‹)
- [ğŸ§© æ³¨æ„äº‹é¡¹](#-æ³¨æ„äº‹é¡¹)
- [ğŸ¤ è´¡çŒ®è€…](#-è´¡çŒ®è€…)
- [ğŸ“„ åè®®](#-åè®®)

## ğŸŒŸ ç®€ä»‹
ğŸ¯ **è¿™æ˜¯ä»€ä¹ˆ?**:
- ğŸ§  è¿™æ˜¯ä¸ºäº†å®Œå…¨ä¸æ‡‚ç¼–ç¨‹çš„**æ–°æ‰‹**è®¾è®¡çš„ï¼Œè®©å…¶æ— éšœç¢çš„ç†è§£ä»£ç æ‰€è¡¨ç¤ºçš„å«ä¹‰çš„ç¨‹åº
- ğŸ”„ è¿™æ˜¯ä¸€ä¸ªå°†ç¼–ç¨‹è¯­è¨€è½¬åŒ–ä¸ºäººç±»å¯ä»¥ç†è§£çš„**è‡ªç„¶è¯­è¨€**çš„ç¨‹åº
- ğŸ“Š è¿™æ˜¯ä¸€ä¸ªå¯ä»¥è®©äººè·Ÿç€ä»£ç **ä¸€åŒæ€è€ƒ**çš„ç¨‹åº

ğŸ”§ **è¿™ä¸æ˜¯ä»€ä¹ˆ**:
- âš¡ï¸ è¿™ä¸æ˜¯ä¸€ä¸ª**ä»£ç è§£é‡Šå™¨**ï¼Œå¹¶ä¸æ˜¯ç”¨æ¥ä¸ºä»£ç **ç”Ÿæˆæ³¨é‡Š**çš„å·¥å…·
- ğŸ§ª è¿™å¹¶ä¸æ˜¯å•çº¯çš„å°†ä»£ç ç”¨è‡ªç„¶è¯­è¨€æè¿°ï¼Œè€Œæ˜¯ç”Ÿæˆä¸€ç§ç”¨æˆ·å¯ä»¥**è·Ÿéšå…¶æ€è€ƒ**çš„è¯­è¨€
- ğŸ“± è¿™ä¸æ˜¯ä¸ºäº†**ç¼–ç¨‹é«˜æ‰‹**è®¾è®¡çš„è½¯ä»¶

ğŸ”§ **å®ç°åŸç†**:

æˆ‘ä»¬ä½¿ç”¨AIå®Œæˆè¿™ä¸ªæ“ä½œï¼Œå¹¶ä¸”å°†åˆ†ä¸ºä¸¤ä¸ªç‰ˆæœ¬:
- **æç¤ºè¯ç‰ˆæœ¬ | prompt edition**:ä½¿ç”¨ä¸€å¥—ä¸ºäº†ä¸åŒæ¨¡å‹è®¾è®¡çš„æç¤ºè¯ï¼Œè¾¾åˆ°æ•ˆæœï¼ˆæ•ˆæœä¸€èˆ¬ï¼‰
- **å¤§æ¨¡å‹ç‰ˆæœ¬ | model edition**: ä½¿ç”¨è®­ç»ƒè¿‡çš„å¤§é¢„è¨€æ¨¡å‹ï¼Œè¾¾åˆ°æ•ˆæœ(æ•ˆæœæ›´å¥½)

## ğŸš€ å¿«é€Ÿä¸Šæ‰‹

æç¤ºè¯ç‰ˆæœ¬ | Prompt Edition

1.Â å®‰è£…ä¾èµ–

pip install openai anthropic transformers

2.Â è®¾ç½®APIå¯†é’¥

import openai
openai.api_key = "ä½ çš„OpenAI_API_KEY"

3.Â ä½¿ç”¨ç¤ºä¾‹

def explain_code(code, model="gpt-4"):
prompt = f"""
è¯·ä»¥åˆå­¦è€…çš„è§’åº¦é€æ­¥è§£é‡Šä»¥ä¸‹ä»£ç ï¼š
{code}

è¦æ±‚ï¼š
1. ç”¨è‡ªç„¶è¯­è¨€æè¿°æ¯éƒ¨åˆ†çš„åŠŸèƒ½
2. ä½¿ç”¨æ¯”å–»å’Œç”Ÿæ´»ä¾‹å­å¸®åŠ©ç†è§£
3. é¿å…ä¸“ä¸šæœ¯è¯­
4. é‡‡ç”¨å¯¹è¯å¼è¯­æ°”
"""
response = openai.ChatCompletion.create(
model=model,
messages=[{"role": "user", "content": prompt}]
)
return response.choices[0].message.content

å¤§æ¨¡å‹ç‰ˆæœ¬ | Model Edition

1.Â æ•°æ®é›†å‡†å¤‡

# ç¤ºä¾‹æ•°æ®æ ¼å¼
dataset = [
{
"code": "for i in range(5):",
"explanation": "è¿™å°±åƒæ•°æ•°ä»0åˆ°4ï¼Œæ€»å…±ä¼šæ•°5æ¬¡ã€‚æƒ³è±¡ä½ åœ¨æ•°è‹¹æœï¼šç¬¬ä¸€ä¸ªã€ç¬¬äºŒä¸ª...ç›´åˆ°ç¬¬äº”ä¸ªã€‚"
},
{
"code": "if x > 10:",
"explanation": "è¿™å°±åƒæ£€æŸ¥ä½ çš„é›¶èŠ±é’±æ˜¯å¦è¶…è¿‡10å…ƒã€‚å¦‚æœæ˜¯ï¼Œå°±å¯ä»¥ä¹°é‚£ä¸ªç©å…·ã€‚"
}
]

2.Â å¾®è°ƒæ¨¡å‹ï¼ˆä½¿ç”¨Hugging Faceï¼‰

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments

model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")
tokenizer = AutoTokenizer.from_pretrained("t5-small")

training_args = TrainingArguments(
output_dir="./results",
per_device_train_batch_size=4,
num_train_epochs=3,
)

trainer = Trainer(
model=model,
args=training_args,
train_dataset=tokenized_dataset,
)

trainer.train()

3.Â ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹

def explain_with_finetuned(code):
inputs = tokenizer(code, return_tensors="pt")
outputs = model.generate(**inputs)
return tokenizer.decode(outputs[0], skip_special_tokens=True)

ä¸¤ç§ç‰ˆæœ¬å¯¹æ¯”

ç‰¹æ€§ æç¤ºè¯ç‰ˆæœ¬ å¤§æ¨¡å‹ç‰ˆæœ¬
å®ç°éš¾åº¦ â­â­ â­â­â­â­
è§£é‡Šè´¨é‡ â­â­â­ â­â­â­â­â­
å“åº”é€Ÿåº¦ â­â­â­â­ â­â­â­
å®šåˆ¶çµæ´»æ€§ â­â­ â­â­â­â­â­
æˆæœ¬ æŒ‰APIè°ƒç”¨ä»˜è´¹ å‰æœŸè®­ç»ƒæˆæœ¬é«˜ï¼ŒåæœŸä½¿ç”¨æˆæœ¬ä½

æ¨èä½¿ç”¨åœºæ™¯

æç¤ºè¯ç‰ˆæœ¬ï¼šå¿«é€ŸéªŒè¯æƒ³æ³•ã€å°è§„æ¨¡ä½¿ç”¨ã€éœ€è¦æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€
å¤§æ¨¡å‹ç‰ˆæœ¬ï¼šéœ€è¦é«˜è´¨é‡è§£é‡Šã€ç‰¹å®šé¢†åŸŸä»£ç ã€é•¿æœŸå¤§è§„æ¨¡ä½¿ç”¨


